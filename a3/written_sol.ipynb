{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Dependency Parsing\n",
    "\n",
    "### 1. Machine Learning & Neural Networks\n",
    "#### (a) Adam Optimizer\n",
    "#### i. Briefly explain how momentum (by keeping track of parameter m) stops the updates from varying as much and why this low variance may be helpful to learning.\n",
    "With simple SGD, the optimization process can be too oscillated and unefficient, i.e. informally speaking, update steps goes zig-zag. However, if we can smooth each update, we can reduce the variance and make the optimation process more efficient. There are several ways to perform smooth function, the benefit of exponential average/ weighted moving average lies in 1) more efficient on memory usage 2) easy to code and implement. In this way, we can control the update by changing $\\beta_{1}$. Note that when $\\beta_{1}=0$, this is essentially SGD.  \n",
    "\n",
    "#### ii. Adam extends the idea of momentum with the track of adaptive learning rates by keeping track of $\\pmb{v}$, a rolling average of the magnitudes of the gradients. Since Adam divides the update by $\\sqrt{\\pmb{v}}$, which of the model parameters will get larger updates? Why might this help with learning?\n",
    "Since $\\pmb{v}$ is a rolling average of the magnitude of the gradient, parameters of the model with smaller magnitude, i.e. with small $\\pmb{v}$ entry, will get larger updates. Essentially, we are cutting out the smaller amount from such parameters during the updates because we devide them by their magnitude. At the same time, the parameters with bigger magnitude will have smaller updates. This approach have two benefits that 1) get recent stagnant parameters update more efficiently along their axis and in turn expedite the optimization 2) smooth some \"large\" updates to get robust performance.\n",
    "\n",
    "#### (b) Dropout\n",
    "#### i. What must $\\gamma$ equal in tems of $p_{drop}$? Briefly justify your answer or show your math derivation using the equation given above.\n",
    "Given that $\\pmb{h}_{drop}=\\gamma\\pmb{d}\\cdot\\pmb{h}$, and $\\mathbb{E}_{p_{drop}}[\\pmb{h}_{drop}]_{i}=h_{i}$ for all $i\\in {1,\\ldots, D_{h}}$, it is straightforward to show that $\\gamma=\\tfrac{1}{1-p_{drop}}$. Intuitively, in training part we dropout some units, but in the test part there is no dropout (as this only increase noises). As a result, we have to fix the units that to make the outputs of the training and testing are similar, and the fix factor is $\\gamma$.\n",
    "\n",
    "#### ii. Why should dropout be applied during training? Why should dropout NOT be applied during evaluation?\n",
    "Why use dropout: dropout is a useful regulization technique to deal with overfitting problem. Intuitively, with dropout, we make the neural net more robust to different dataset (especially the datasets that it never \"see\" before).\n",
    "\n",
    "Why NOT use it during evaluation: when we evaluate the neural net, we are concerned with how well the model is with unseen data. In this case, if we use dropout, we are \"thinning\" the net which in many cases only increase the noise to the predictions and dampen the accuracy of the model, and we cannot fairly assess the generalization power of the model.\n",
    "\n",
    "### 2. Neural Transition-based Dependency Parsing\n",
    "#### (a) Go through the sequence of transitions needed for parsing the sentence \"I parsed this sentence correctly\".\n",
    "Stack | Buffer | New dependency | Transition |\n",
    "--- | --- | --- | --- |\n",
    "[ROOT] | [I, parsed, this, sentence, correctly]| | Initial Configuration \n",
    "[ROOT, I] | [parsed, this, sentence, correctly]| | SHIFT\n",
    "[ROOT, I, parsed] | [this, sentence, correctly]| | SHIFT\n",
    "[ROOT, parsed] | [this, sentence, correctly]| parsed$\\rightarrow$I | LEFT-ARC\n",
    "[ROOT, parsed, this] | [sentence, correctly]| | SHIFT\n",
    "[ROOT, parsed, this, sentence] | [correctly]| | SHIFT\n",
    "[ROOT, parsed, sentence] | [correctly]| sentence$\\rightarrow$this | LIFT-ARC\n",
    "[ROOT, parsed] | [correctly]| parsed$\\rightarrow$sentence | RIGHT-ARC\n",
    "[ROOT, parsed, correctly] | []| | SHIFT\n",
    "[ROOT, parsed] | []| parsed$\\rightarrow$correctly | RIGHT-ARC\n",
    "[ROOT] | [] | ROOT$\\rightarrow$parsed | RIGHT-ARC\n",
    "\n",
    "#### (b) A sentence containing n words will be parsed in how many steps (in terms of n)?\n",
    "O(n). In particular, each word must goes from buffer to stack exactly once, so we have n SHIFT steps. Similarly, each word must be removed from the stack exactly once, thus we have n LEFT-ARC or RIGHT-ARC steps. In total, this gives 2n time, i.e. O(n). \n",
    "\n",
    "#### (f)\n",
    "i. \n",
    "+ Error type: Verb Phrase Attachment Error\n",
    "+ Incorrect dependency: wedding $\\rightarrow$ fearing\n",
    "+ Correct dependency: heading $\\rightarrow$ fearing\n",
    "\n",
    "ii.\n",
    "+ Error type: Coordination Attachment Error\n",
    "+ Incorrect dependency: makes -> rescue\n",
    "+ Correct dependency: rush -> rescue\n",
    "\n",
    "iii.\n",
    "+ Error type: Prepositional Phrase Attachment Error\n",
    "+ Incorrect dependency: named -> midland\n",
    "+ Correct dependency: guy -> midland\n",
    "\n",
    "iv.\n",
    "+ Error type: Modifier Attachment Error\n",
    "+ Incorrect dependency: elements -> most\n",
    "+ Correct dependency: crucial -> most"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
