{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS 224N- Assignment 5 (2021)\n",
    "---\n",
    "\n",
    "### Attention exploration\n",
    "\n",
    "#### (a) Copying in attention: Describe (in one sentence) what properties of the inputs to the attention operation would result in the output c being approximately equal to vj for some j ∈ {1, . . . , n}. Specifically, what must be true about the query q, the values {v1, . . . , vn} and/or the keys {k1, . . . , kn}?\n",
    "\n",
    "Sol: To achieve the goal, we must have $k^{T}_{j}q >> k^{T}_{i}q,\\; i\\neq j$.\n",
    "\n",
    "#### (b) An average of two: Give an expression for a query vector q such that the output c is approximately equal to the average of va and vb, that is, 1/2(va + vb).\n",
    "\n",
    "Sol: $q = t(u_{a}+u_{b}),\\;t>>0$.\n",
    "\n",
    "#### (c) Drawbacks of single-headed attention:\n",
    "i. Design a query q in terms of the µi such that as before, c ≈ 1/2(va + vb), and provide a brief argument as to why it works.\n",
    "\n",
    "Sol: $q = t(u_{a}+u_{b}), \\; t>>0$.\n",
    "\n",
    "ii. Though single-headed attention is resistant to small perturbations in the keys, some types of larger perturbations may pose a bigger issue. When you sample {k1, . . . , kn} multiple times, and use the q vector that you defined in part i., what qualitatively do you expect the vector c will look like for different samples?\n",
    "\n",
    "Sol: it can be shown that $k_{a} \\sim N(\\mu_{a}, \\alpha I+1/2(\\mu_{a}\\mu_{a}^{T})),$ and for vanishingly small $\\alpha$: $k_{a} \\approx \\epsilon_{a}\\mu_{a}, \\epsilon_{a}\\sim N(1, 1/2),$ when $q=t(u_{a}+u_{b}), t>> 0$, we have $k^{T}_{i}q\\approx0\\; for\\;i \\notin \\{a, b\\}$, $k_{a}^{T}q\\approx \\epsilon_{a}t$, $k^{T}_{b}q\\approx \\epsilon_{b}t$. Thus we have $c \\rightarrow v_{a}$.\n",
    "\n",
    "#### (d) Benefits of multi-headed attention:\n",
    "i.\n",
    "\n",
    "Sol: $q_{a}=t_{1}\\mu_{a}, t_{1}>>0$, $q_{b}=t_{2}\\mu_{b}, t_{2}>>0$.\n",
    "\n",
    "ii.\n",
    "\n",
    "Sol: $c\\approx \\tfrac{1}{2}(v_{a}+v_{b})$.\n",
    "\n",
    "#### (e) Key-Query-Value self-attention in neural networks:\n",
    "i. $c_{2}\\approx u_{a}$. It is impossible for $c_{2}$ to approximate $u_{b}$ by just adding either $u_{d}$ or $u_{c}$ to $x_{2}$. Since $u_{d}$ and $u_{b}$ will increase equally in $c_{2}$.\n",
    "\n",
    "ii. Let \n",
    "$$V = (u_{b}u_{b}^{T}-u_{c}u_{c}^{T})\\cdot\\tfrac{1}{\\beta^{2}}$$\n",
    "$$K = I$$\n",
    "$$Q = (u_{d}u_{a}^{T}-u_{c}u_{d}^{T})\\cdot\\tfrac{1}{\\beta^{2}}$$\n",
    "\n",
    "It can be showed that we can have the desired results in this way.\n",
    "\n",
    "### 2. Pretrained Transformer models and knowledge access\n",
    "#### (g)\n",
    "ii. What might the synthesizer self-attention not be able to do, in a single layer, what the key-query-value self-attention can do?\n",
    "\n",
    "Sol: the synthesizer self-attention cannot capture the similarity between the embeddings, i.e. the context information.\n",
    "\n",
    "### 3. Considerations in pretrained knowledge\n",
    "#### (a) Succinctly explain why the pretrained (vanilla) model was able to achieve an accuracy of above 10%, whereas the non-pretrained model was not.\n",
    "Sol: The pretrained model contains extra information from the extra corpus, which can be transfered.\n",
    "\n",
    "#### (b) Come up with two reasons why this indeterminacy of model behavior may cause concern for suc applications.\n",
    "\n",
    "Sol: 1. Bias and stereotype; 2. It can generate some results that seem to be realistic but actually are totally wrong!!\n",
    "\n",
    "#### (c) \n",
    "\n",
    "Sol: For example, it can generate the birthplace of some already known person with similar name. Whereas the similarity of name has nothing to do with the birthplace."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
