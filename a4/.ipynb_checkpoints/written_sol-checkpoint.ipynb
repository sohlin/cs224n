{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS 224n: Assignment \\#4\n",
    "### 1.Neural Machine Translation with RNNs\n",
    "#### (g) The generate sent masks() function in nmt model.py produces a tensor called enc masks. It has shape (batch size, max source sentence length) and contains 1s in positions corresponding to ‘pad’ tokens in the input, and 0s for non-pad tokens. Look at how the masks are used during the attention computation in the step() function. First explain (in around three sentences) what effect the masks have on the entire attention computation. Then explain (in one or two sentences) why it is necessary to use the masks in this way. \n",
    "\n",
    "Sol: Essentially, we use pad to make each sentence in a batch share the same length, which indicates that the 'pad' tokens have no information. Thus we give them a very small value, i.e. $-\\inf$, in calculating the $e_{t}$ attention scores. Intuitively, it is said to not to focus on the pad token. Otherwise, padding token would somewhat dampen the attention distribution.\n",
    "\n",
    "#### (h) Please report the model’s corpus BLEU Score.\n",
    "Sol: The Corpus BLEU is 23.63.\n",
    "\n",
    "#### (i) In class, we learned about dot product attention, multiplicative attention, and additive attention. As a reminder, dot product attention is $e_{t,i} = s^{T}_{t}h_{i}$, multiplicative attention is $e_{t,i} = s^{T}_{t} Wh_{i}$, and additive attention is $e_{t,i} = v\n",
    "^{T}tanh(W_{1}h_{i} + W_{2}s_{t})$. Explain one advantage and one disadvantage of dot product attention compared to multiplicative attention. Explain one advantage and one disadvantage of additive attention compared to multiplicative attention.\n",
    "Sol:\n",
    "\n",
    " Attention |Advantage| Disadvantage\n",
    ":--: | :--: | :--:\n",
    "__Dot product attention__ | Easy to compute, memory efficient, and more intuitive, i.e. any source hidden vector that is orthogonal to target hidden vector will be \"washed out\" | It requires that $s_{t},h_{i}\\in\\mathbb{R}_{m}$, i.e. have the same dimension, rigid\n",
    "__Multiplicative attention__ | Efficient to compute, more flexible than dot product attention | Still not flexible enough than additive attention\n",
    "__additive attention__ | Both target and source hidden vector share their own project matrix, i.e. more flexibility | Slower to compute than the other two attentions\n",
    "\n",
    "### 2. Analyzing NMT Systems\n",
    "#### (a) In part 1, we modeled our NMT problem at a subword-level. That is, given a sentence in the source language, we looked up subword components from an embeddings matrix. Alternatively, we could have modeled the NMT problem at the word-level, by looking up whole words from the embeddings matrix. Why might it be important to model our Cherokee-to-English NMT problem at the subword-level vs. the whole word-level?\n",
    "Sol: Since Cherokee is a polysynthetic language, where a single word may function as a whole sentence, i.e. language characterized by complex words consisting of several morphemes. In this case, it is better to model at the subword level, because if we model it at the word-level, 1) the embedding size would be much bigger as to better characterize the sentence, 2) embedding for a whole sentence would dampen the performance of the model since each single word can function as a sentence, and all words together would lead to much complex linguistic results.\n",
    "\n",
    "#### (b) Character-level and subword embeddings are often smaller than whole word embeddings. In 1-2 sentences, explain one reason why this might be the case.\n",
    "Sol: The information character-level and subword embeddings reflects is fewer than the whole word embedding.\n",
    "\n",
    "####  (c) One challenge of training successful NMT models is lack of language data, particularly for resource-scarce languages like Cherokee. One way of addressing this challenge is with multilingual training, where we train our NMT on multiple languages (including Cherokee). You can read more about multilingual training here. How does multilingual training help in improving NMT performance with low-resource languages?\n",
    "Sol: Many languages share some common features, e.g. grammer structures. In this case, when training a model that is based on multilingual features, the model is capable of capturing the representational similarity across a large body of languages. Thus the model can be generalized to fit the resource-scarce languages like Cherokee.\n",
    "\n",
    "#### (d) Here we present three examples of errors we found in the outputs of our NMT model (which is the same as the one you just trained). The errors are underlined in the NMT translation sentence. For each example of a source sentence, reference (i.e., ‘gold’) English translation, and NMT (i.e., ‘model’) English translation, please:\n",
    "1. Provide possible reason(s) why the model may have made the error (either due to a specific linguistic construct or a specific model limitation).\n",
    "2. Describe one possible way we might alter the NMT system to fix the observed error. There are more than one possible fixes for an error. For example, it could be tweaking the size of the hidden layers or changing the attention mechanism.\n",
    "\n",
    "i. \n",
    "+ Source Translation: Yona utsesdo ustiyegv anitsilvsgi digvtanv uwoduisdei.\n",
    "+ Reference Translation: Fern had a crown of daisies in her hair.\n",
    "+ NMT Translation: Fern had $\\underline{her\\;hair}$ with her hair.\n",
    "\n",
    "Sol: \n",
    "Error: Model limitation. It didn't capture the linguistic construct with slang using.\n",
    "Fix: Increase more similar linguistic data and increase the size of hidden layer.\n",
    "\n",
    "ii. \n",
    "+ Source Sentence: Ulihelisdi nigalisda.\n",
    "+ Reference Translation: She is very excited.\n",
    "+ NMT Translation: $\\underline{It’s}$ joy.\n",
    "\n",
    "Sol:\n",
    "Error: Model limitation. It didn't capture the relationship in certain context.\n",
    "Fix: Increase the sentence size to include more context information. Increase more similar source data.\n",
    "\n",
    "iii. \n",
    "+ Source Sentence: Tsesdi hana yitsadawoesdi usdi atsadi! \n",
    "+ Reference Translation: Don’t swim there, Littlefish!\n",
    "+ NMT Translation: Don’t know how $\\underline{a\\;small\\;fish}$!\n",
    "\n",
    "Sol: \n",
    "Error: Model limitation. The word embedding didn't know the other meaning of the slang.\n",
    "Fix: Increase the size of the embedding. Change the attention mechanism to addictive attention as to include more context information.\n",
    "\n",
    "#### (f) BLEU\n",
    "It is usually calculated across the entire test set, but here we will consider BLEU defined for a single example.2 Suppose we have a source sentence s, a set of k reference translations $r_{1},\\ldots, r_{k}$, and a candidate translation c. To compute the BLEU score of c, we first compute the modified n-gram precision $p_{n}$ of c, for each of n = 1, 2, 3, 4, where n is the n in n-gram:\n",
    "$$p_{n}=\\frac{\\sum_{ngram\\in c}min(max_{i=1,\\ldots,k}(Count_{r_{i}}(ngram)), Count_{c}(ngram))}{\\sum_{ngram\\in c}Count_{c}(ngram)}$$ \n",
    "\n",
    "Next, we compute the brevity penalty BP. Let len(c) be the length of c and let len(r) be the length of the reference translation that is closest to len(c) (in the case of two equally-close reference translation lengths, choose len(r) as the shorter one).\n",
    "$$\n",
    "BP=\\begin{cases}\n",
    "\\begin{split}\n",
    "1, &len(c)\\geq len(r) \\\\\n",
    "   exp(1-len(r)/len(c)), &len(c)<len(r)\n",
    "\\end{split}\n",
    "\\end{cases}\n",
    "$$ \n",
    "\n",
    "Lastly, the BLEU score for candidate c with respect to r1, . . . , rk is:\n",
    "$$BLEU=BP\\cdot exp(\\sum_{i=1}^{4}\\lambda_{i}log(p_{i}))$$ \n",
    "\n",
    "where λ1, λ2, λ3, λ4 are weights that sum to 1. The log here is natural log.\n",
    "##### i. Please consider this example from Spanish:\n",
    "+ Source Sentence s: el amor todo lo puede\n",
    "+ Reference Translation r1: love can always find a way\n",
    "+ Reference Translation r2: love makes anything possible\n",
    "+ NMT Translation c1: the love can always do\n",
    "+ NMT Translation c2: love can make anything possible \n",
    "\n",
    "Please compute the BLEU scores for $c_{1}$ and $c_{2}$. Let $λ_{i} = 0.5$ for $i \\in {1, 2}$ and $λ_{i} = 0$ for $i \\in {3, 4}$ (this means we ignore 3-grams and 4-grams, i.e., don’t compute p3 or p4). When computing BLEU scores, show your working (i.e., show your computed values for p1, p2, len(c), len(r) and BP). Note that the BLEU scores can be expressed between 0 and 1 or between 0 and 100. The code is using the 0 to 100 scale while in this question we are using the 0 to 1 scale. Which of the two NMT translations is considered the better translation according to the BLEU Score? Do you agree that it is the better translation? \n",
    "\n",
    "Sol:\n",
    "$c_{1}$: $p_{1}=\\tfrac{0+1+1+1+0}{1+1+1+1+1}=0.6$, $\\tfrac{0+1+1+0}{1+1+1+1}=0.5$, since len(c)=5$\\geq$len(r)=len($r_{2}$)=4, so BP=1. It can be showed that $BLEU_{1}=1\\cdot exp(0.5\\cdot(log(0.6)+log(0.5)))=0.5477$. \n",
    "\n",
    "$c_{2}$: $p_{1}=\\tfrac{1+1+0+1+1}{1+1+1+1+1}=0.8$, $\\tfrac{1+0+0+1}{1+1+1+1}=0.5$, since len(c)=5$\\geq$len(r)=len($r_{2}$)=4, so BP=1. It can be showed that $BLEU_{2}=1\\cdot exp(0.5\\cdot(log(0.8)+log(0.5)))=0.6325$. \n",
    "\n",
    "$c_{2}$ is considered to be the better translation with HIGHER BLEU. I agree with the result.\n",
    "\n",
    "##### ii.  Our hard drive was corrupted and we lost Reference Translation $r_{2}$. Please recompute BLEU scores for $c_{1}$ and $c_{2}$, this time with respect to $r_{1}$ only. Which of the two NMT translations now receives the higher BLEU score? Do you agree that it is the better translation?\n",
    "\n",
    "Sol:\n",
    "$c_{1}$: $p_{1}=\\tfrac{0+1+1+1+0}{1+1+1+1+1}=0.6$, $\\tfrac{0+1+1+0}{1+1+1+1}=0.5$, since len(c)=5<len(r)=len($r_{1}$)=6, so BP=exp(1-6/5). It can be showed that $BLEU_{1}=exp(-0.2)\\cdot exp(0.5\\cdot(log(0.6)+log(0.5)))=0.4484$. \n",
    "\n",
    "$c_{2}$: $p_{1}=\\tfrac{1+1+0+0+0}{1+1+1+1+1}=0.4$, $\\tfrac{1+0+0+0}{1+1+1+1}=0.25$, since len(c)=5$<$len(r)=len($r_{1}$)=6, so BP=exp(1-6/5). It can be showed that $BLEU_{2}=exp(-0.2)\\cdot exp(0.5\\cdot(log(0.4)+log(0.25)))=0.2589$. \n",
    "\n",
    "$c_{1}$ is considered to be the better translation with HIGHER BLEU. I disagree with the result.\n",
    "\n",
    "##### iii.  Due to data availability, NMT systems are often evaluated with respect to only a single reference translation. Please explain (in a few sentences) why this may be problematic.\n",
    "Sol: The translation are somewhat subjective translations, and different representation can have the same meaning. If NMT systems are often evaluated with respect to only a single reference translation, then good translation might receive a low BLEU score due to little n-gram overlaps.\n",
    "\n",
    "##### iv. List two advantages and two disadvantages of BLEU, compared to human evaluation, as an evaluation metric for Machine Translation.\n",
    "Sol: \n",
    "\n",
    "Advantages | Disadvantages\n",
    ":--: | :--:\n",
    "Fully automated and quantitative evaluation, no subjective influence. | Only measures n-grams overlaps, lacks ability to evaluate structures and positions\n",
    "Fast to compute as opposed to forcing human to interpret both source and target languages | No measuring sematics, log, corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
