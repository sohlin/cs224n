{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 for CS224n\n",
    "\n",
    "### (a)\n",
    "Show that naive-softmax loss is the same as the cross-entropy loss between $y$ and $\\hat{y}$, i.e. show that:\n",
    "$$-\\sum_{w\\in Vocab}y_{w}log(\\hat{y}_{w})=-log(\\hat{y}_{o})$$ \n",
    "\n",
    "Sol: Because $\\pmb{y}$ is a one-hot vector with a 1 for the true outside word o, and 0 everywhere else. Thus the LHS is essentially $-(0log(\\hat{y}_{1})+\\ldots+1log(\\hat{y}_{o})+\\ldots+0log(\\hat{y}_{|V|})=-log(\\hat{y}_{o})))$. \n",
    "\n",
    "### (b)\n",
    "Compute the partial derivative $J_{naive-softmax}(uc,o,U)=-logP(O=o|C=c)$ w.r.t $v_{c}$.\n",
    "\n",
    "Sol: It is defined that $-logP(O=o|C=c)=-log\\tfrac{e^{u_{o}^{T}v_{c}}}{\\sum_{w\\in Vocab}e^{u_{w}^{T}v_{c}}}$, thus we can compute that \n",
    "$$\n",
    "\\begin{split}\n",
    "\\tfrac{\\partial J}{\\partial v_{c}} &= -u_{o}+\\tfrac{1}{\\sum_{w\\in Vocab}e^{u_{w}^{T}v_{c}}}\\sum_{w\\in Vocab}(e^{u_{w}^{T}v_{c}}u_{w}) \\\\\n",
    "&= -u_{o}+\\sum_{w\\in Vocab}(\\tfrac{e^{u_{w}^{T}v_{c}}}{\\sum_{w\\in Vocab}e^{u_{w}^{T}v_{c}}}u_{w}) \\\\\n",
    "&= -u_{o}+\\sum_{w\\in Vocab}(P(u_{w}|v_{c})u_{w}) \\\\\n",
    "&= -u_{o}+\\sum_{w\\in Vocab}(\\hat{y}_{w}u_{w})\n",
    "\\end{split}\n",
    "$$ \n",
    "\n",
    "### (c)\n",
    "Compute the partial derivative $J_{naive-softmax}(uc,o,U)=-logP(O=o|C=c)$ w.r.t $u_{w}$.\n",
    "\n",
    "Sol: \n",
    "\n",
    "**Case 1**: $w=o$. We have \n",
    "$$\n",
    "\\begin{split}\n",
    "\\tfrac{\\partial J}{\\partial u_{w=o}} &= -v_{c}+\\tfrac{1}{\\sum_{w\\in Vocab}e^{u_{w}^{T}v_{c}}}(e^{u_{o}^{T}v_{c}}v_{c}) \\\\\n",
    "&= v_{c}(\\hat{y}_{o}-1)\n",
    "\\end{split}\n",
    "$$ \n",
    "\n",
    "**Case 2**: $w\\neq o$. We have\n",
    "$$\n",
    "\\begin{split}\n",
    "\\tfrac{\\partial J}{\\partial u_{w\\neq o}} &= \\tfrac{1}{\\sum_{w\\in Vocab}e^{u_{w}^{T}v_{c}}}(e^{u_{w\\neq o}^{T}v_{c}}v_{c}) \\\\\n",
    "&= v_{c}\\hat{y}_{w\\neq o}\n",
    "\\end{split}\n",
    "$$ \n",
    "\n",
    "### (d)\n",
    "Compute the partial derivative $J_{naive-softmax}(uc,o,U)=-logP(O=o|C=c)$ w.r.t U.\n",
    "\n",
    "Sol: It is straightforward to show that $\\tfrac{\\partial J}{\\partial U} = [\\tfrac{\\partial J}{\\partial u_{1}},\\ldots,\\tfrac{\\partial J}{\\partial u_{|Vocab|}}]_{k\\times|Vocab|},$ where k is the dimension of word vector. \n",
    "\n",
    "### (e)\n",
    "Compute the derivative of sigmoid function $\\sigma(x)=\\tfrac{1}{1+e^{-x}}$ w.r.t x, where x is a scalar.\n",
    "\n",
    "Sol: It is straightforward to show that $\\tfrac{d \\sigma(x)}{d x} = \\tfrac{e^{-x}}{1+e^{-x}}=\\sigma(x)(1-\\sigma(x))$.\n",
    "\n",
    "### (f)\n",
    "Now consider Negative Sampling Loss that $J_{neg-sample}(v_{c},o,U)=-log(\\sigma(u_{o}^{T}v_{c}))-\\sum_{k=1}^{K}log(\\sigma(-u_{k}^{T}v_{c}))$. Compute the partial derivative of it w.r.t $v_{c},u_{o},u_{k}$. \n",
    "\n",
    "Sol: It can be showed that\n",
    "$$\n",
    "\\begin{split}\n",
    "\\tfrac{\\partial J}{\\partial v_{c}} &= -\\tfrac{1}{\\sigma(u_{o}^{T}v_{c})}\\sigma'(u_{o}^{T}v_{c})u_{o}-\\sum_{k}\\tfrac{\\sigma'(-u_{k}^{T}v_{c})}{\\sigma(-u_{k}^{T}v_{c})}(-u_{k}) \\\\\n",
    "&= (\\sigma(u_{o}^{T}v_{c})-1)u_{o}+\\sum_{k}(1-\\sigma(-u_{k}^{T}v_{c}))u_{k}\n",
    "\\end{split}\n",
    "$$ \n",
    "$$\n",
    "\\tfrac{\\partial J}{\\partial u_{o}} = (\\sigma(u_{o}^{T}v_{c})-1)v_{c}\n",
    "$$\n",
    "$$\n",
    "\\tfrac{\\partial J}{\\partial u_{k}} = (1-\\sigma(-u_{k}^{T}v_{c})v_{c}\n",
    "$$\n",
    "\n",
    "It can be seen that with Negative Sampling, it is much more efficient to compute the gradient, since we only need K samples (O(K)) while the naive softmax needs the whole vocab (O(|Vocab|)).\n",
    "\n",
    "### (g)\n",
    "Now consider Negative Sampling Loss that $J_{neg-sample}(v_{c},o,U)=-log(\\sigma(u_{o}^{T}v_{c}))-\\sum_{k=1}^{K}log(\\sigma(-u_{k}^{T}v_{c}))$. Compute the partial derivative of it w.r.t $u_{k}$ without assumption that the K negative samples are distinct.\n",
    "\n",
    "Sol: $$\n",
    "\\tfrac{\\partial J}{\\partial u_{k}} = \\sum_{j=k}(1-\\sigma(-u_{j}^{T}v_{c})v_{c}\n",
    "$$\n",
    "\n",
    "### (h)\n",
    "Compute the following three partial derivatives.\n",
    "\n",
    "(i) $\\partial J(v_{c},w_{t-m},\\ldots,w_{t+m},U) /\\partial U$.\n",
    "\n",
    "Sol: $\\partial J_{skip-gram}(v_{c},w_{t-m},\\ldots,w_{t+m},U)/\\partial U$ = $\\sum{-m\\leq j\\leq m, j\\neq 0} J_{skip-gram}(v_{c},w_{j},U)/\\partial U$.\n",
    "\n",
    "(ii)$\\partial J(v_{c},w_{t-m},\\ldots,w_{t+m},U)/\\partial v_{c}$.\n",
    "\n",
    "Sol: $\\partial J(v_{c},w_{t-m},\\ldots,w_{t+m},U)/\\partial v_{c}$ = $\\sum{-m\\leq j\\leq m, j\\neq 0} J_{skip-gram}(v_{c},w_{j},U)/\\partial v_{c}$.\n",
    "\n",
    "(iii) $\\partial J(v_{c},w_{t-m},\\ldots,w_{t+m},U)/\\partial v_{w}$ when $w\\neq c$.\n",
    "\n",
    "Sol: 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code result\n",
    "\n",
    "![image info](./word_vectors.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
